{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO0uGtb99HYYWIfegtEXIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amir-asari/Qwen-VL-Basic/blob/main/Qwen_Zeroshot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2SwLMGTycXR"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --- 1. Environment Setup and Installation ---\n",
        "This block installs/upgrades libraries and forces a runtime restart.\n",
        "\n",
        "This ensures bitsandbytes loads correctly for 4-bit quantization.\n"
      ],
      "metadata": {
        "id": "7Vhmz8ZzykIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Check if a specific environment variable is set after the first run\n",
        "    if not os.environ.get('QWEN_VL_RESTARTED'):\n",
        "        print(\"Installing necessary packages...\")\n",
        "        # 1. Install transformers from source for Qwen2.5-VL compatibility.\n",
        "        # 2. Install/upgrade bitsandbytes for the latest 4-bit quantization features.\n",
        "        # 3. Install necessary utility libraries, including 'datasets' for CIFAR-100.\n",
        "        !pip install -q git+https://github.com/huggingface/transformers.git accelerate\n",
        "        !pip install -q -U bitsandbytes\n",
        "        !pip install -q qwen-vl-utils pillow requests datasets # Added datasets\n",
        "\n",
        "        # Set an environment variable to prevent restart on the second run\n",
        "        os.environ['QWEN_VL_RESTARTED'] = 'true'\n",
        "\n",
        "        print(\"Installation complete. ***Please restart the runtime to load new packages!***\")\n",
        "        # --- IMPORTANT FIX: Restart the runtime after installing/upgrading packages ---\n",
        "        os._exit(00)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Initial installation failed: {e}\")"
      ],
      "metadata": {
        "id": "0-Uta2WEyw6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --- 2. Imports and Model Loading (with 4-bit Quantization) ---"
      ],
      "metadata": {
        "id": "9I0oqsIjy55t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "import random # Added for sampling the dataset\n",
        "from PIL import Image\n",
        "from datasets import load_dataset # Added for loading CIFAR-100\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "# Suppress minor warnings for a clean output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Switched to the smaller 3B model for compatibility and speed on T4 GPUs.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float16\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "print(f\"--- Environment Setup ---\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loading Model: {MODEL_ID} (with 4-bit Quantization)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "model = None\n",
        "try:\n",
        "    # 1. Define 4-bit quantization configuration\n",
        "    # This is the key to preventing CUDA Out of Memory errors\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # 2. Load the Model and Processor\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "    # Load model using the 4-bit configuration\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "    print(\"Model loaded successfully in 4-bit precision.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or libraries: {e}\")\n",
        "    print(\"Please ensure you have a T4 GPU enabled and the runtime has been restarted after installation.\")\n",
        "    model = None"
      ],
      "metadata": {
        "id": "Jrcze3aizAH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --- 3. Zero-Shot Classification Function ---"
      ],
      "metadata": {
        "id": "8QIQ4_fnzNne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_classify(image: Image.Image, candidate_labels: list):\n",
        "    \"\"\"\n",
        "    Performs zero-shot classification by generating a prompt that includes the\n",
        "    list of candidate labels (VQA format) for the provided PIL Image object.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return \"Model failed to load. Cannot run inference.\"\n",
        "\n",
        "    labels_str = \", \".join([f\"'{label}'\" for label in candidate_labels])\n",
        "\n",
        "    # The classification prompt is framed as a VQA task\n",
        "    question = (\n",
        "        f\"What is the most accurate classification of the object shown in this image? \"\n",
        "        f\"Choose only one answer from the following candidates: {labels_str}. State only the chosen label.\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Running Classification ---\")\n",
        "    print(f\"Image object passed (Size: {image.size})\") # Now printing image size\n",
        "    print(f\"Candidate Labels: {len(candidate_labels)} total\")\n",
        "\n",
        "    # 1. Construct the chat template, passing the PIL Image object\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 2. Process the input and generate response\n",
        "    try:\n",
        "        inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate response\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,\n",
        "            max_new_tokens=20, # Only need a short output (the chosen label)\n",
        "        )\n",
        "\n",
        "        response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the assistant's response part\n",
        "        assistant_tag = \"<|im_start|>assistant\\n\"\n",
        "        if assistant_tag in response_text:\n",
        "            response_text = response_text.split(assistant_tag)[-1].strip()\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during model inference: {e}\""
      ],
      "metadata": {
        "id": "fnRPDD1mzStZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --- 4. CIFAR-100 Dataset Loading and Execution ---"
      ],
      "metadata": {
        "id": "AG7G1vLHzZLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading CIFAR-100 Dataset (Test Split) ---\")\n",
        "# Load the test split of CIFAR-100 (5000 images, 100 classes)\n",
        "dataset = load_dataset('cifar100', split='test')\n",
        "\n",
        "# Extract the 100 class names to use as candidate labels\n",
        "candidate_labels = dataset.features['fine_label'].names\n",
        "\n",
        "print(f\"Total CIFAR-100 Test Images: {len(dataset)}\")\n",
        "print(f\"Number of Candidate Labels: {len(candidate_labels)}\")\n",
        "print(f\"Example Labels: {candidate_labels[:5]}...\")\n",
        "\n",
        "# Select a small, fixed set of random samples for demonstration purposes\n",
        "random.seed(42) # Ensure the same samples are picked every time\n",
        "sample_indices = random.sample(range(len(dataset)), 5)\n",
        "\n",
        "print(\"\\n--- Starting Zero-Shot Classification on Sample Images ---\")\n",
        "\n",
        "for i, index in enumerate(sample_indices):\n",
        "    item = dataset[index]\n",
        "\n",
        "    # Extract the PIL Image object and the ground truth label name\n",
        "    image_pil = item['img']\n",
        "    expected_label = candidate_labels[item['fine_label']]\n",
        "\n",
        "    # Perform classification using the PIL Image object\n",
        "    predicted_label = zero_shot_classify(image_pil, candidate_labels)\n",
        "\n",
        "    print(f\"\\n[CIFAR-100 Test {i+1} / {len(sample_indices)}]\")\n",
        "    print(f\"Expected Label:  {expected_label}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "pQ1s-P3Wzd_w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}