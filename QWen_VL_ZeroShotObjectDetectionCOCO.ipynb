{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzlf3MqfFoQoyYQVnuBI1f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amir-asari/Qwen-VL-Basic/blob/main/QWen_VL_ZeroShotObjectDetectionCOCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74XMEt_Oifwu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### --- 1. Environment Setup and Installation ---"
      ],
      "metadata": {
        "id": "-3hUT8vsi0RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This block installs/upgrades libraries and forces a runtime restart.\n",
        "print(\"Installing necessary packages...\")\n",
        "\n",
        "\n",
        "!pip install -q git+[https://github.com/huggingface/transformers.git](https://github.com/huggingface/transformers.git) accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q qwen-vl-utils pillow"
      ],
      "metadata": {
        "id": "txSAWGEyi1HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### --- 2. Imports and Model Loading (with 4-bit Quantization) ---"
      ],
      "metadata": {
        "id": "4fzUo-7Ii9Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "import json\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig\n",
        "from IPython.display import display\n",
        "\n",
        "# --- NEW IMPORTS for Detection Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# ---------------------------------------------\n",
        "\n",
        "# Suppress minor warnings for a clean output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Switched to the smaller 3B model for compatibility and speed on T4 GPUs.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float16\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "print(f\"--- Environment Setup ---\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Loading Model: {MODEL_ID} (with 4-bit Quantization)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "model = None\n",
        "try:\n",
        "    # 1. Define 4-bit quantization configuration\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # 2. Load the Model and Processor\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "    # Load model using the 4-bit configuration\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=\"auto\",\n",
        "    ).eval()\n",
        "    print(\"Model loaded successfully in 4-bit precision.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or libraries: {e}\")\n",
        "    print(\"Please ensure you have a T4 GPU enabled and the runtime has been restarted after installation.\")\n",
        "    model = None\n"
      ],
      "metadata": {
        "id": "GoRiSnbDjEl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### --- 3. Zero-Shot Detection Function ---"
      ],
      "metadata": {
        "id": "CbcqIhWyjJYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_detect(image_url: str, query_object: str):\n",
        "    \"\"\"\n",
        "    Performs zero-shot object detection by instructing the Qwen model to output\n",
        "    a JSON structure containing bounding box coordinates for a specific object.\n",
        "\n",
        "    Returns: A dictionary containing the PIL Image and a list of detected objects.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        return {\"error\": \"Model failed to load.\"}\n",
        "\n",
        "    # 1. Download and load the image\n",
        "    try:\n",
        "        # --- FIX: Added User-Agent header to spoof a browser and bypass 403 Forbidden errors ---\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(image_url, stream=True, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error loading image from URL: {e}\"}\n",
        "\n",
        "    # The prompt explicitly asks for JSON output with normalized bounding boxes (0-1000).\n",
        "    # Qwen-VL is trained to understand grounding requests in this format.\n",
        "    prompt_instruction = (\n",
        "        f\"Detect all instances of '{query_object}' in the image. \"\n",
        "        f\"Output the results as a single JSON array of objects. \"\n",
        "        f\"Each object must have two keys: 'box' (a list of 4 normalized integers: [x_min, y_min, x_max, y_max] where the maximum value is 1000) and 'label' (string). \"\n",
        "        f\"DO NOT include any conversational text or explanation, ONLY output the JSON array.\"\n",
        "    )\n",
        "\n",
        "    # 2. Define the conversation\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a professional object detection model. Your output must ONLY be a JSON array.\"}],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt_instruction}\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 3. Process input and generate JSON text\n",
        "    try:\n",
        "        inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,\n",
        "            max_new_tokens=256, # Increased token limit to allow for complex JSON output\n",
        "        )\n",
        "\n",
        "        response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Robustly clean the output to isolate the JSON block\n",
        "        assistant_marker = \"\\nassistant\\n\"\n",
        "        if assistant_marker in response_text:\n",
        "            json_text = response_text.split(assistant_marker)[-1].strip()\n",
        "        else:\n",
        "            json_text = response_text.strip()\n",
        "\n",
        "        # Regex to find the JSON array structure [ ... ]\n",
        "        match = re.search(r'\\[.*\\]', json_text, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "            return {\"image\": image, \"detections\": [], \"error\": f\"JSON array not found in output: {json_text[:100]}...\"}\n",
        "\n",
        "        # 4. Parse the JSON output\n",
        "        try:\n",
        "            detections = json.loads(match.group(0))\n",
        "            return {\"image\": image, \"detections\": detections, \"error\": None}\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"image\": image, \"detections\": [], \"error\": f\"Failed to parse valid JSON: {match.group(0)[:100]}...\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"image\": image, \"detections\": [], \"error\": f\"Inference exception: {e}\"}\n",
        "\n",
        "\n",
        "def draw_detections(image: Image.Image, detections: list, title: str):\n",
        "    \"\"\"Draws bounding boxes and labels on the image and displays it.\"\"\"\n",
        "\n",
        "    if not detections:\n",
        "        print(f\"No detections found for: {title}\")\n",
        "        display(image)\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    img_width, img_height = image.size\n",
        "\n",
        "    # Iterate through each detected object\n",
        "    for det in detections:\n",
        "        try:\n",
        "            label = det.get('label', 'Unknown')\n",
        "            # Normalized coordinates (0-1000) from Qwen\n",
        "            x_min_norm, y_min_norm, x_max_norm, y_max_norm = det['box']\n",
        "\n",
        "            # Convert normalized coordinates (0-1000) back to pixel values\n",
        "            x_min = x_min_norm * img_width / 1000\n",
        "            y_min = y_min_norm * img_height / 1000\n",
        "            width = (x_max_norm - x_min_norm) * img_width / 1000\n",
        "            height = (y_max_norm - y_min_norm) * img_height / 1000\n",
        "\n",
        "            # Create a Rectangle patch\n",
        "            rect = patches.Rectangle(\n",
        "                (x_min, y_min),\n",
        "                width,\n",
        "                height,\n",
        "                linewidth=2,\n",
        "                edgecolor='r',\n",
        "                facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Add label text\n",
        "            plt.text(\n",
        "                x_min,\n",
        "                y_min - 5,\n",
        "                label,\n",
        "                color='white',\n",
        "                fontsize=12,\n",
        "                bbox=dict(facecolor='r', alpha=0.7, edgecolor='none')\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping malformed detection: {det}. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    ax.axis('off') # Hide axes\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tBjhStzOjSn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####--- 4. Zero-Shot Detection Dataset and Execution ---"
      ],
      "metadata": {
        "id": "3sL2hnGhjlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Starting Zero-Shot Object Detection ---\")\n",
        "\n",
        "# Define a dataset using stable URLs from the COCO dataset (known images for detection tasks)\n",
        "detection_dataset = [\n",
        "    {\n",
        "        # COCO Image: A crowd of people on a beach with an umbrella\n",
        "        \"url\": \"http://images.cocodataset.org/val2017/000000000139.jpg\",\n",
        "        \"query\": \"all people and the umbrella\",\n",
        "        \"title\": \"COCO Sample: Beach Crowd\"\n",
        "    },\n",
        "    {\n",
        "        # COCO Image: A stop sign in a snowy environment\n",
        "        \"url\": \"http://images.cocodataset.org/val2017/000000000285.jpg\",\n",
        "        \"query\": \"the traffic sign and the road\",\n",
        "        \"title\": \"COCO Sample: Snowy Stop Sign\"\n",
        "    },\n",
        "    {\n",
        "        # COCO Image: A large elephant next to a small table\n",
        "        \"url\": \"http://images.cocodataset.org/val2017/000000000785.jpg\",\n",
        "        \"query\": \"the elephant and the potted plant\",\n",
        "        \"title\": \"COCO Sample: Elephant and Plant\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Track metrics\n",
        "total_runs = 0\n",
        "successful_parses = 0\n",
        "\n",
        "for item in detection_dataset:\n",
        "    total_runs += 1\n",
        "\n",
        "    print(f\"\\nDetecting objects in: {item['title']} | Query: '{item['query']}'\")\n",
        "\n",
        "    # Perform detection\n",
        "    result = zero_shot_detect(item[\"url\"], item[\"query\"])\n",
        "\n",
        "    if result.get('error'):\n",
        "        print(f\"STATUS: Failed. {result['error']}\")\n",
        "    else:\n",
        "        successful_parses += 1\n",
        "        print(f\"STATUS: Success. Found {len(result['detections'])} objects.\")\n",
        "        draw_detections(result['image'], result['detections'], f\"{item['title']} - Query: {item['query']}\")"
      ],
      "metadata": {
        "id": "izEvqcVHjsA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### --- 5. Performance Summary ---"
      ],
      "metadata": {
        "id": "hFrbF1Lqj1f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"--- 5. Object Detection Performance Summary ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if total_runs > 0:\n",
        "    parse_success_rate = (successful_parses / total_runs) * 100\n",
        "    print(f\"Total Test Runs: {total_runs}\")\n",
        "    print(f\"Successful JSON Parses: {successful_parses}\")\n",
        "    print(f\"JSON Parsing Success Rate: {parse_success_rate:.2f}%\")\n",
        "    print(\"\\nNote: True detection accuracy (IoU) is not calculated here, as it requires ground-truth bounding box data.\")\n",
        "else:\n",
        "    print(\"No test runs were executed.\")"
      ],
      "metadata": {
        "id": "5DehFq3ij6IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FOv55Vu9j-bX"
      }
    }
  ]
}